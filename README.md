# AWS EKS Cluster with Pulumi

This Pulumi application provisions an AWS Elastic Kubernetes Service (EKS) cluster using Python. The infrastructure follows best practices with DRY principles, helper functions for common operations, and centralized configuration management. It includes VPC, managed node groups, CloudWatch logging, and will support Application Load Balancer (ALB) integration.

## Prerequisites

1.  **Pulumi CLI:** Install Pulumi:
    - **macOS (Homebrew):** `brew install pulumi`
    - **Linux/Windows:** `curl -fsSL https://get.pulumi.com | sh`
2.  **Python 3.8+:** Required for the Pulumi Python runtime
3.  **uv:** Fast Python package manager:
    - **macOS (Homebrew):** `brew install uv`
    - **Linux/Windows:** `curl -LsSf https://astral.sh/uv/install.sh | sh`
4.  **AWS CLI:** Install and configure the AWS CLI with credentials that have permissions to create EKS clusters and related resources (VPC, IAM roles, EC2 instances, etc.).
5.  **kubectl:** Install kubectl to interact with the Kubernetes cluster.
6.  **Task CLI:** Install [Task](https://taskfile.dev/) for running automation commands.
7.  **Domain name:** Optional for ALB setup - can use default AWS domain for testing or custom domain for production.

## Directory Structure

```
aws-eks/
├── pulumi/
│   ├── __main__.py       # Main Pulumi application with EKS cluster configuration
│   ├── requirements.txt  # Python dependencies for Pulumi
│   ├── requirements.lock # Locked dependency versions (generated by uv)
│   ├── Pulumi.yaml       # Pulumi project configuration
│   ├── Pulumi.staging.yaml    # Stack-specific configuration for staging
│   └── Pulumi.production.yaml # Stack-specific configuration for production
├── Taskfile.yml          # Task definitions for common operations
├── ALB-SETUP.md          # ALB and Route 53 setup guide (to be updated for Pulumi)
├── CLAUDE.md             # Claude AI assistant guidance
└── README.md             # This file - complete documentation and setup guide
```

## Configuration Files

*   `pulumi/__main__.py`: Main Pulumi application containing helper functions and infrastructure definition. Creates VPC (using `pulumi-awsx`), EKS cluster, managed node groups, IAM roles, and VPC endpoints. Uses best practices with DRY principles and centralized configuration.
*   `pulumi/requirements.txt`: Python dependencies including `pulumi`, `pulumi-aws`, and `pulumi-awsx`.
*   `pulumi/requirements.lock`: Locked dependency versions generated by uv for reproducible builds.
*   `pulumi/Pulumi.yaml`: Pulumi project configuration defining the project name, runtime, and description.
*   `pulumi/Pulumi.staging.yaml`: Stack-specific configuration for staging environment.
*   `pulumi/Pulumi.production.yaml`: Stack-specific configuration for production environment.
*   `Taskfile.yml`: Contains [Task](https://taskfile.dev/) definitions for automating common operations like configuring `kubectl`, scaling node groups, and managing Pulumi stacks.
*   `ALB-SETUP.md`: Detailed guide for setting up and configuring the Application Load Balancer with Route 53 (to be updated for Pulumi).
*   `CLAUDE.md`: Guidance for Claude AI assistant when working with this repository.

## Configuration Management

All configuration is managed through Pulumi's native config system:

### AWS Configuration
- AWS region: `task config:set env=staging key=aws:region value=us-east-1`

### Cluster Configuration
- `node-disk-size`: EBS volume size for worker nodes (default: 100)
- `node-ami-type`: Node group AMI type (default: AL2_x86_64)
- `vpc-cidr`: VPC CIDR block (default: 10.0.0.0/16)
- `cluster-name-prefix`: Cluster name prefix (default: eks-cluster)

### Feature Toggles
- `enable-vpc-endpoints`: Enable/disable VPC endpoints (default: true)
- `enable-cluster-logging`: Enable/disable CloudWatch logging (default: true)
- `enable-public-endpoint`: Enable/disable public API endpoint (default: true)
- `enable-private-endpoint`: Enable/disable private API endpoint (default: true)

### Environment-Specific Settings
- `staging-instance-type`: Instance type for staging (default: t3.medium)
- `staging-min-size`: Min nodes for staging (default: 1)
- `staging-max-size`: Max nodes for staging (default: 3)
- `staging-desired-size`: Desired nodes for staging (default: 2)
- `staging-nat-gateways`: NAT gateways for staging (default: 1)
- `production-*`: Same settings for production with different defaults

### Default Environment Configurations

#### Staging Environment (Default)
- **Instance Type:** t3.medium
- **Node Group:** 1-3 nodes (2 desired)
- **NAT Gateways:** 1 (cost optimization)

#### Production Environment
- **Instance Type:** m5.large
- **Node Group:** 2-10 nodes (3 desired)
- **NAT Gateways:** 3 (high availability)

## Usage

### First-Time Setup

1.  **Install Python dependencies:**
    ```bash
    task setup
    ```
    This will install all required Python packages using `uv` for fast dependency resolution.

2.  **Configure stack settings:**
    ```bash
    # Set basic configuration for staging
    task config:set env=staging key=project-name value="MyEKSProject"
    task config:set env=staging key=cost-center value="engineering"

    # Configure production with different settings
    task config:set env=production key=production-instance-type value="m5.xlarge"
    task config:set env=production key=production-desired-size value="5"
    ```

3.  **Initialize Pulumi stacks:**
    ```bash
    # For staging environment
    task pulumi:init env=staging

    # For production environment
    task pulumi:init env=production
    ```

### Deploying Infrastructure

1.  **Review and update the `.env` file for your target environment.**
    For example, open `pulumi/.env` and review the default values. You might need to adjust `ENVIRONMENT`, `VPC_CIDR`, and region-specific settings to suit your needs.

2.  **Preview the deployment:**
    ```bash
    task preview env=staging
    # or
    task preview env=production
    ```

3.  **Deploy the infrastructure:**
    ```bash
    task deploy env=staging
    # or
    task deploy env=production
    ```

4.  **Configure kubectl:**
    Update your default kubeconfig file (usually at `~/.kube/config`).
    ```bash
    task eks:kubeconfig env=staging
    ```
    Verify cluster access.
    ```bash
    kubectl get nodes
    kubectl get svc
    ```

## ALB Setup - Two Configuration Modes

The ALB setup supports two flexible modes for different use cases:

### Mode 1: Default AWS ALB Domain (Quick Testing)

Perfect for development and testing without needing a custom domain:

```bash
# Edit terraform/staging.tfvars
use_default_domain = true
enable_alb         = true
```

**Features:**
- Subdomain routing: `http://api.k8s-default-xxx.us-east-1.elb.amazonaws.com`
- No SSL setup required
- No custom domain needed
- Perfect for development/testing

### Mode 2: Custom Domain with Route 53 (Production)

For production setups with your own domain:

```bash
# Edit terraform/staging.tfvars
use_default_domain = false
enable_alb         = true
domain_name        = "yourdomain.com"  # Replace with your actual domain
```

**Features:**
- Subdomain routing: `https://api.yourdomain.com`
- Automatic SSL certificate via AWS Certificate Manager
- Route 53 DNS management
- Professional URLs for production

### Deployment Steps

1. **Deploy infrastructure with ALB:**
   ```bash
   task plan env=staging
   task apply env=staging
   ```

2. **For Custom Domain Mode:** Configure DNS nameservers at your domain registrar:
   ```bash
   # Get nameservers for your domain registrar
   task alb:dns:instructions
   ```

3. **Check ALB status and get URLs:**
   ```bash
   task alb:status
   ```

### Service URLs After Setup

**Default AWS Domain Mode:**
- API service: `http://api.k8s-default-xxx.us-east-1.elb.amazonaws.com`
- ArgoCD service: `http://argocd.k8s-default-xxx.us-east-1.elb.amazonaws.com`

**Custom Domain Mode:**
- API service: `https://api.yourdomain.com`
- ArgoCD service: `https://argocd.yourdomain.com`

For detailed ALB setup instructions, see [ALB-SETUP.md](ALB-SETUP.md).

## Scaling the Cluster

To scale the cluster, run:
```bash
task eks:scale env=staging desiredSize=3
# or with all parameters
task eks:scale env=staging desiredSize=3 minSize=1 maxSize=5
```

## Destroying the Cluster

To tear down the resources, run:
```bash
task destroy env=staging
# or
task destroy env=production
```

## Code Quality Features

This implementation follows software engineering best practices:

### DRY (Don't Repeat Yourself) Principles
- **Helper Functions**: Reusable functions for IAM role creation and policy attachment
- **Centralized Configuration**: All settings managed through `config_vars` dictionary
- **Environment Abstraction**: Single codebase handles multiple environments

### Easy Maintenance
- **Modular Design**: Separated concerns with dedicated functions
- **Clear Structure**: Logical grouping of related resources
- **Consistent Naming**: Standardized resource naming patterns
- **Type Hints**: Python type annotations for better code clarity

### Configuration Management
- **Pulumi Config**: Native configuration management with encryption for secrets
- **Stack Isolation**: Environment-specific configuration per stack
- **Validation**: Input validation and sensible defaults
- **Documentation**: Comprehensive inline comments and README

### Modern Tooling
- **uv Package Manager**: Lightning-fast Python package installation and dependency resolution
- **Dependency Locking**: Reproducible builds with `requirements.lock` file
- **Virtual Environment**: Automatic virtual environment management
- **Pulumi State**: Automatic state management with collaboration features

## Useful Commands

The Pulumi code follows best practices:
```bash
# View stack outputs
task outputs env=staging

# Check stack status
task status env=staging

# View stack information
task info env=staging

# Validate configuration
task validate env=staging

# Update dependencies and create lock file
task pulumi:sync

# View and manage configuration
task config:list env=staging
task config:set env=staging key=node-disk-size value=200
```

### Configuration Management

This project uses Pulumi's native configuration system:

```bash
# List all configuration for a stack
task config:list env=staging

# Set configuration values
task config:set env=staging key=vpc-cidr value="172.16.0.0/16"
task config:set env=production key=production-instance-type value="m5.xlarge"

# Set secrets (encrypted)
task config:set-secret env=staging key=admin-user-arn value="arn:aws:iam::123:user/admin"

# Get specific configuration value
task config:get env=staging key=node-disk-size
```

### Dependency Management with uv

This project uses [uv](https://github.com/astral-sh/uv) for fast and reliable Python package management:

```bash
# Install dependencies (done automatically by task setup)
task pulumi:deps

# Update and lock dependencies
task pulumi:sync
```

## Task Reference

View all available tasks:
```bash
task --list
```

## Important Notes

*   **Kubernetes Version:** The `cluster_version` variable in `variables.tf` is set to a default (e.g., `1.32`). Always check the [official AWS EKS documentation](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html) for the latest supported Kubernetes versions and update accordingly.
*   **VPC and Subnets:** This configuration creates a new VPC specifically for the EKS cluster using the `terraform-aws-modules/vpc/aws` module. This module handles the creation of public and private subnets across specified availability zones, NAT gateways (configurable, e.g., one per AZ or a single NAT gateway), and ensures all resources are tagged appropriately for EKS compatibility (e.g., `kubernetes.io/cluster/<cluster_name>=shared`).
*   **IAM Permissions:** The AWS credentials used to run Terraform need sufficient permissions to create and manage EKS clusters, IAM roles, EC2 instances, security groups, and other related resources.
*   **EKS Access Entries:** Cluster access for IAM principals is managed via EKS Access Entries, which is the successor to the `aws-auth` ConfigMap method. This configuration uses `API_AND_CONFIG_MAP` mode to support both modern access entries and node group bootstrapping.
*   **Worker Node Labeling:** Managed node groups are automatically labeled with `node-role.kubernetes.io/worker = "worker"`. This standard label helps `kubectl` display node roles correctly and can be used for scheduling workloads.
*   **ALB Configuration:** Two modes available - default AWS domain for testing (subdomain routing without SSL) or custom domain for production (subdomain routing with SSL and Route 53).
*   **State Management:**
    *   This project uses an AWS S3 remote backend with S3-native state locking (via `use_lockfile = true` in `backend.tf`)
    *   The S3 bucket is managed separately in `terraform-backend/` to avoid circular dependencies
    *   The bucket includes versioning and lifecycle policies (retains 2 newest noncurrent versions, older versions deleted after 90 days)
    *   Workspaces are used to manage multiple environments (staging/production) with isolated state files
*   **Cost:** Running an EKS cluster and associated resources will incur costs on your AWS bill. Make sure to destroy resources when they are no longer needed if you are experimenting.
